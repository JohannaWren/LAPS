{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import netCDF4\n",
    "from netCDF4 import Dataset\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from scipy import interpolate\n",
    "import math \n",
    "from pathlib import Path\n",
    "\n",
    "from parcels import FieldSet,NestedField, ParticleSet, JITParticle, ScipyParticle, AdvectionRK4,DiffusionUniformKh, Variable, Field,GeographicPolar,Geographic\n",
    "from datetime import timedelta as timedelta\n",
    "import datetime\n",
    "from parcels.tools.converters import TimeConverter\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.lines import Line2D\n",
    "from copy import copy\n",
    "from os.path import isfile\n",
    "import pytz\n",
    "from os import path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Delete and Egg movement Kernels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Delete Error\n",
    "def DeleteErrorParticle(particle, fieldset, time):\n",
    "    if particle.state == StatusCode.ErrorOutOfBounds:\n",
    "        particle.delete()\n",
    "\n",
    "# Delete Kernel\n",
    "def DeleteParticle(particle, fieldset, time):\n",
    "    print('deleted particle')\n",
    "    particle.delete()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def EggHatchingMovement(particle, fieldset, time):\n",
    "    eggdepth = 0.25 # egg depth in m\n",
    "    eggtime=1*86400\n",
    "    vertical_speed = 0.02  # sink and rise speed in m/s\n",
    "    drifttime1 =  30* 86400  # time of deep drift in seconds\n",
    "\n",
    "\n",
    "    if particle.cycle_phase == 0:\n",
    "        # Phase 0:particle is an egg and has yet to hatch\n",
    "        particle.age += particle.dt\n",
    "        particle.depth=eggdepth \n",
    "        if particle.age >= eggtime:\n",
    "            particle.cycle_phase = 1 # phase 1 is sinking after \"hatching\"\n",
    "            \n",
    "    elif particle.cycle_phase == 1:\n",
    "        # Phase 1: Sinking with vertical_speed until depth is driftdepth1\n",
    "        particle_ddepth += vertical_speed * particle.dt\n",
    "        particle.age += particle.dt # added this in because the particles are sinking to various/deep depths and will need more time than other particles (this may get resolved by increasing the run time but better to do this I think)\n",
    "        if particle.depth + particle_ddepth >= particle.driftlayer:\n",
    "            particle_ddepth = particle.driftlayer - particle.depth\n",
    "            particle.cycle_phase = 2 # phase 2 is drifting in the first 25% of PLD\n",
    "\n",
    "    elif particle.cycle_phase == 2:\n",
    "        # Phase 2: Drifting at larval depth\n",
    "        particle.age += particle.dt\n",
    "        particle.depth=particle.driftlayer \n",
    "\n",
    "        \n",
    "def AgeDelete(particle, fieldset, time):\n",
    "    if particle.age > (31*86400):\n",
    "        #print(\"soy vieja\")\n",
    "        particle.delete()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# first round of spawning\n",
    "#startDate = '1992-02-14' \n",
    "#endDate = '1992-06-30'\n",
    "\n",
    "# second round of spawning\n",
    "#startDate = '1992-06-14' \n",
    "#endDate = '1992-10-30'\n",
    "\n",
    "# 1992 for testing purposes\n",
    "# Define start and end dates\n",
    "startDate = '1992-10-02'\n",
    "endDate = '1993-02-28'\n",
    "\n",
    "# Wake Hycom Files are labeled by their time since epoch\n",
    "# getting the desired start/end dates in time since epoch\n",
    "desired_startdate = datetime.datetime(1992, 10, 2, tzinfo=pytz.utc)  # Year, Month, Day\n",
    "desired_enddate = datetime.datetime(1993, 3, 1, tzinfo=pytz.utc) \n",
    "\n",
    "startseconds_since_epoch = desired_startdate.timestamp()\n",
    "endseconds_since_epoch = desired_enddate.timestamp()\n",
    "startseconds_since_epoch = int(startseconds_since_epoch)\n",
    "endseconds_since_epoch = int(endseconds_since_epoch)\n",
    "\n",
    "print(startseconds_since_epoch)\n",
    "print(endseconds_since_epoch)\n",
    "\n",
    "interval = 10800 # 3 hr interval in seconds \n",
    "print(\"start loading in files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#file = glob.glob(\"/home/esd_data/Hycom_Wake/HYCOM_Wake_*.nc\")\n",
    "# create list of file paths between the start and end time in seconds sincce epoch \n",
    "expected_files = [f\"/home/esd_data/Hycom_Wake/HYCOM_Wake_{seconds}.nc\" for seconds in range(startseconds_since_epoch, endseconds_since_epoch, interval)]\n",
    "#print(expected_files[0:5])\n",
    "# Filter the list to just the files that actually exist. (This is for when you remove NA files)\n",
    "actual_files = [f for f in expected_files if isfile(f)]\n",
    "\n",
    "ds1 = xr.open_mfdataset(actual_files)  # this puts the opendap data into a xarray dataset\n",
    "#print(ds1)\n",
    "myDat1 = ds1.sel(**{'TIME': slice(startDate,endDate)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "variables = {'U': 'WATER_U',\n",
    "             'V': 'WATER_V'}\n",
    "dimensions = {'lon': 'LONGITUDE4001_4563',\n",
    "              'lat': 'LATITUDE1064_1376',\n",
    "              'time': 'TIME',\n",
    "              'depth': 'LEV1_20'}\n",
    "\n",
    "fieldset = FieldSet.from_xarray_dataset(myDat1, variables, dimensions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "kh = 10.0   # This is the eddy diffusivity in m2/s\n",
    "fieldset.add_constant_field('Kh_zonal', kh, mesh='spherical')\n",
    "#zonal follows lat\n",
    "fieldset.add_constant_field('Kh_meridional', kh, mesh='spherical') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DisplacementParticle(JITParticle):\n",
    "    #dU = Variable('dU', to_write = False)\n",
    "    #dV = Variable('dV', to_write = False)\n",
    "    #d2s = Variable('d2s', initial=1e3, to_write = False)\n",
    "    age = Variable('age', dtype=np.float32, initial= 0., to_write = False)\n",
    "    cycle_phase=Variable('cycle_phase', dtype=np.float32, initial=0., to_write = False)\n",
    "    releaseSite = Variable('releaseSite', dtype=np.int32, to_write = False)\n",
    "    #distance = Variable('distance', dtype=np.int32, initial=0.) # not calculating distance for now but left this in\n",
    "    #prev_lat = Variable('prev_lat', initial=0., to_write=False)  \n",
    "    #prev_lon = Variable('prev_lon', initial=0., to_write=False)\n",
    "    f = Variable('f', dtype=np.int32, to_write = False)\n",
    "    driftlayer = Variable('driftlayer', dtype=np.int32, to_write = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "source_loc = pd.read_csv('/home/gmukai/Desktop/Wake_Parcel_Files/Wakedeeper_Taongi_Bikar_releasesites.csv', header=None)\n",
    "# Number of particle released per location\n",
    "npart_perlayer = 1\n",
    "npart = 10*npart_perlayer\n",
    "\n",
    "# Release location from the file read in above\n",
    "lon = np.repeat(source_loc[0], npart)\n",
    "lat = np.repeat(source_loc[1],npart)\n",
    "site = np.repeat(source_loc[2],npart)\n",
    "# Start date for release. Since we are releasing every set number of days the repeatdt version was simplest\n",
    "#start_date = 0\n",
    "dlayer = [0.25]*(len(source_loc)*npart)\n",
    "driftlayers = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]  # Define the layers\n",
    "repeating_layers = driftlayers * (len(source_loc)*npart_perlayer)\n",
    "\n",
    "repeatdt = timedelta(days=1)\n",
    "\n",
    "print(\"called in release points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"create pset now\")\n",
    "pset = ParticleSet.from_list(fieldset=fieldset, pclass=DisplacementParticle,lon=lon,lat=lat,releaseSite=site,\n",
    "                             depth=dlayer, repeatdt=repeatdt, driftlayer = repeating_layers)\n",
    "print(\"create kernels\")\n",
    "\n",
    "kernels = [EggHatchingMovement, displace, AdvectionRK4, DiffusionUniformKh, set_displacement, AgeDelete, DeleteErrorParticle]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "output_file = pset.ParticleFile(name=\"Wake_1992_n10_n1perlayer_kh10_Oct_Feb_bounce_Bumphead6_17.zarr\", outputdt=timedelta(hours=0.5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt=timedelta(minutes=10)\n",
    "\n",
    "run_days = 76\n",
    "print(\"start execute\")\n",
    "pset.execute(kernels,\n",
    "            runtime=timedelta(days=run_days),\n",
    "            dt=model_dt, \n",
    "            output_file=output_file)\n",
    "pset.repeatdt = None\n",
    "\n",
    "pset.execute(kernels,\n",
    "            runtime=timedelta(days=31+1),\n",
    "            dt=model_dt, \n",
    "            output_file=output_file)\n",
    "\n",
    "#data_xarray = xr.open_zarr(\"Wake_1992_n10_kh10_Oct_Feb_bounce5_2_2.zarr\")\n",
    "#data_xarray.to_netcdf(\"Wake_1992_n10_kh10_Oct_Feb_bounce5_2_2.nc\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
